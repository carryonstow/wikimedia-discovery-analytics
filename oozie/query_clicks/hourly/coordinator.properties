# Configures a coordinator to manage automatically joining
# cirrus request logs against webrequest logs to record
# which links were clicked in full text search results.
#
# Any of the following properties are overidable with -D.
# Usage:
#   oozie job -Duser=$USER -Dstart_time=2015-12-01T00:00Z -submit \
#       -config oozie/query_clicks/hourly/coordinator.properties
#
# NOTE:  Both *_oozie_directory must be synced to HDFS so that all relevant
#        .xml files exist there when this job is submitted.

# Base path in HDFS to this repository oozie files.
# Other files will be used relative to this path.
discovery_oozie_directory             = ${name_node}/wmf/discovery/current/oozie

# Base path in HDFS to the analytics team oozie files.
# Other files will be used relative to this path
refinery_directory                    = ${name_node}/wmf/refinery/current
analytics_oozie_directory             = ${refinery_directory}/oozie
# HDFS path to artifacts that will be used by this job.
# E.g. refinery-hive.jar should exist here.
analytics_artifacts_directory         = ${refinery_directory}/artifacts
# Version of Hive UDF jar to import
refinery_jar_version                  = 0.0.39

name_node                             = hdfs://analytics-hadoop
job_tracker                           = resourcemanager.analytics.eqiad.wmnet:8032
queue_name                            = default

hive2_jdbc_url                        = jdbc:hive2://an-coord1001.eqiad.wmnet:10000/default

user                                  = hdfs

# HDFS path to coordinator to run to generate query_clicks_hourly
coordinator_file                      = ${discovery_oozie_directory}/query_clicks/hourly/coordinator.xml

# HDFS path to workflow to run to generate query_clicks_hourly
workflow_file                         = ${discovery_oozie_directory}/query_clicks/hourly/workflow.xml

# HDFS path to webrequest dataset definitions
webrequest_datasets_file              = ${analytics_oozie_directory}/webrequest/datasets.xml
webrequest_data_directory             = ${name_node}/wmf/data/wmf/webrequest
webrequest_table                      = wmf.webrequest

# HDFS path to discovery dataset definitions
query_clicks_datasets_file            = ${discovery_oozie_directory}/query_clicks/datasets.xml
discovery_data_directory              = ${name_node}/wmf/data/discovery
eventgate_data_directory              = ${name_node}/wmf/data/event
cirrus_event_table                    = event.mediawiki_cirrussearch_request
query_clicks_hourly_table             = discovery.query_clicks_hourly

# Fully qualified name of the mediawiki_namespace_map table in hive
namespace_map_table                   = wmf_raw.mediawiki_project_namespace_map
namespace_map_snapshot_id             = 2017-01_private

# Initial import time of the query clicks dataset.
start_time                        = 2016-12-08T00:00Z

# Time to stop running this coordinator.  Year 3000 == never!
stop_time                         = 3000-01-01T00:00Z

# HDFS path to workflow to mark a directory as done
mark_directory_done_workflow_file = ${analytics_oozie_directory}/util/mark_directory_done/workflow.xml

# Workflow to send an error email
send_error_email_workflow_file    = ${analytics_oozie_directory}/util/send_error_email/workflow.xml
send_error_email_address          = ebernhardson@wikimedia.org

# HDFS path to hive-site.xml file.  This is needed to run hive actions.
hive_site_xml                     = ${name_node}/user/hive/hive-site.xml

# Coordintator to start.
oozie.coord.application.path      = ${coordinator_file}
oozie.use.system.libpath          = true
oozie.action.external.stats.write = true
