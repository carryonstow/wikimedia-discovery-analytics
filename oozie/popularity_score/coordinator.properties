# Configures a coordinator to manage automatically aggregating
# popularity_score from the pageview_hourly table.
#
# Any of the following properties are overidable with -D.
# Usage:
#   oozie job -Duser=$USER -Dstart_time=2015-12-01T00:00Z -submit \
#       -config oozie/popularity_score/coordinator.properties
#
# NOTE:  Both *_oozie_directory must be synced to HDFS so that all relevant
#        .xml files exist there when this job is submitted.

# Base path in HDFS to this repository oozie files.
# Other files will be used relative to this path.
discovery_oozie_directory         = ${name_node}/wmf/discovery/current/oozie

# Base path in HDFS to the analytics team oozie files.
# Other files will be used relative to this path
analytics_oozie_directory         = ${name_node}/wmf/refinery/current/oozie

name_node                         = hdfs://analytics-hadoop
job_tracker                       = resourcemanager.analytics.eqiad.wmnet:8032
queue_name                        = default

user                              = hdfs

# HDFS path to coordinator to run to generate popularity_score
coordinator_file                  = ${discovery_oozie_directory}/popularity_score/coordinator.xml

# HDFS path to workflow to run to generate popularity_score.
workflow_file                     = ${discovery_oozie_directory}/popularity_score/workflow.xml

# HDFS path to pageview dataset definitions
pageview_datasets_file            = ${analytics_oozie_directory}/pageview/datasets.xml
pageview_data_directory           = ${name_node}/wmf/data/wmf/pageview
pageview_hourly_data_directory    = ${pageview_data_directory}/hourly
pageview_hourly_table             = wmf.pageview_hourly

# HDFS path to popularity score dataset definitions
discovery_datasets_file           = ${discovery_oozie_directory}/datasets.xml
discovery_data_directory          = ${name_node}/wmf/data/discovery
popularity_score_data_directory   = ${discovery_data_directory}/popularity_score
popularity_score_table            = discovery.popularity_score

# Number of days to aggregate page views over. This is also the frequency
# the workflow will be run.
days_aggregated                   = 7

# Initial import time of the popularity score dataset. This is one week after the page_id
# field was added to pageview_hourly.
start_time                        = 2015-12-08T00:00Z

# Time to stop running this coordinator.  Year 3000 == never!
stop_time                         = 3000-01-01T00:00Z

# HDFS path to workflow to add partition to hive
add_partition_workflow_file       = ${analytics_oozie_directory}/util/hive/partition/add/workflow.xml

# HDFS path to workflow to mark a directory as done
mark_directory_done_workflow_file = ${analytics_oozie_directory}/util/mark_directory_done/workflow.xml

# Workflow to send an error email
send_error_email_workflow_file    = ${analytics_oozie_directory}/util/send_error_email/workflow.xml

# HDFS path to hive-site.xml file.  This is needed to run hive actions.
hive_site_xml                     = ${name_node}/user/hive/hive-site.xml

# Spark assembly jar path needs to be given to spark conf
spark_assembly_jar                = ${name_node}/user/spark/share/lib/spark-assembly.jar
# Hive library jars path to spark to use HiveContext
hive_lib_path                     = /usr/lib/hive/lib/*

# Spark load settings - Better to set them here, configurable through oozie
# when spark alocates this resource, it's not shareable anymore (even if only
# one worker blocks the whole thing), so we prefer to be small-ish instead 
# of big-ish :) with this setiings, job runs in less than 1/2h
spark_number_executors            = 16
spark_executor_memory             = 3G
spark_driver_memory               = 2G

# Record version to keep track of changes
record_version                    = 0.0.1

# Coordintator to start.
oozie.coord.application.path      = ${coordinator_file}
oozie.use.system.libpath          = true
oozie.action.external.stats.write = true
